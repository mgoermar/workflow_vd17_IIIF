# workflow_vd17_IIIF
*Autor:* Maximilian Görmar (goermar@hab.de)

Using bibliographic metadata in your research process can be messy and complicated. But it can be of great use to obtain data on a large scale. One can, for example, directly analyse the metadata quantitatively or one can use it to compile a text corpus for further analysis. The workflow and XSLT-script provided here can be used for the later use case. Its aim is to facilitate the gathering of research data from already existing ressources, especially for historical analysis. By using Linked Open Data and open standards it tries to contribute to more sustainable research practices in the digital humanities.

# How does it work?
In order to use the script you have to run a query via an SRU-API of a library or union catalogue. In the use case the worflow and script were originally developed this was the [VD17-catalogue](http://www.vd17.de/), the retrospective German national bibliography for the 17th century. The aim was to download a corpus of a specific text genre for a limited time period, particularly [Messrelationen](https://en.wikipedia.org/wiki/Messrelation) published between 1618 and 1629. The URL for the respective SRU-query looks like this: https://sru.k10plus.de/vd17?version=2.0&operation=searchRetrieve&query=pica.gat=messrelation+and+pica.jhr=1618-1629&maximumRecords=500&startRecord=1&recordSchema=dc. If you want to search for another genre you can do so by replacing "messrelation" in the URL by another term from the controlled vocabulary of genres of the *Arbeitsgemeinschaft Alte Drucke* ([AAD genres](https://uri.gbv.de/terminology/aadgenres/)). Accordingly you can also search for other years or delete the part "+and+pica.jhr=1618-1629" in the URL, if you want to search the entire VD17.

Currently, the script only works with digitized texts from the Bayerische Staatsbibliothek München (BSB). In the future it would be highly desirable to include other libraries and adapt the script accordingly. For now the restrict the query above to the holdings of the BSB by adding "+and+pica.bib=40004" to the URL. "40004" is the the External Library Number (ELN) of the BSB. These numbers are not very well documented. So the easiest way to find them is to search for the titles held by specific libraries via the links on the [respective overview on the VD17-homepage](http://www.vd17.de/en/about-the-project/membership/).  The links look like that: [https://kxp.k10plus.de/DB=1.28/CMD?&ACT=SRCHA&TRM=bbg+a*+and+bib+40004](https://kxp.k10plus.de/DB=1.28/CMD?&ACT=SRCHA&TRM=bbg+a*+and+bib+40004) (BSB) or [https://kxp.k10plus.de/DB=1.28/CMD?&ACT=SRCHA&TRM=bbg+a*+and+bib+0023](https://kxp.k10plus.de/DB=1.28/CMD?&ACT=SRCHA&TRM=bbg+a*+and+bib+0023) (Herzog August Bibliothek Wolfenbüttel). Accordingly, the example query-URL for the use case above looks like this: https://sru.k10plus.de/vd17?version=2.0&operation=searchRetrieve&query=pica.gat=messrelation+and+pica.jhr=1618-1629+and+pica.bib=40004&maximumRecords=50&startRecord=1&recordSchema=dc.

We get a result in XML which should be stored locally to process it further (see the example file [sru_query_vd17_messrelationen.xml](https://github.com/mgoermar/workflow_vd17_IIIF/blob/main/sru_query_vd17_messrelationen.xml). In our case, we use a XSLT-script on the XML to download OCR-texts via the IIIF-API of the BSB and save them in txt-files. To execute the script there are several ways:
  - You can use the Saxon XSLT-processor which comes in different versions. If you use the [Java-Version](https://www.saxonica.com/download/java.xml) you can run the script with the following command: `java -jar your-saxon-installation-path\saxon-he-12.4.jar -s:sru_query_vd17_messrelationen.xml -xsl:download_ocr_bsb.xsl`.
  - You can also use the open source editor [Visual Studio Code](https://code.visualstudio.com/) with the extension [xslt-transform](https://github.com/sagneessens/vscode-xslt-transform). You still need Java and the respective Saxon-Version installed, but you can use it directly from the editor.
  - You use a commercial XML-editor like [Oxygen XML Editor](https://www.oxygenxml.com/) where the Saxon processor is built-in and easy to use by defining transformation scenarios.

Once executed, the script iterates first through all `<dc:identifier>`-elements in the SRU-search results which contain “mdz” to filter the URN-links of the BSB.[^1] From these links, e.g., http://mdz-nbn-resolving.de/urn:nbn:de:bvb:12-bsb10503608-2, the identifiers of the digital objects we are interested in are extracted. In the example the identifier is the following string near the end: “bsb10503608”. This identifier is also used to access the respective IIIF-manifest via a link which may look like this: https://api.digitale-sammlungen.de/iiif/presentation/v2/bsb10503608/manifest.

The IIIF-manifest is a core element of the [International Image Interoperability Framework, short: IIIF](https://iiif.io/), an open standard to make digital images available around which several APIs were developed by a con-sortium of leading cultural institutions including the BSB. The manifest is a JSON-file which contains all relevant information about a digital object to be displayed in a IIIF-viewer. This in-cludes metadata and links to image files as well as to files containing OCR-text. The XSLT-script parses the manifest to find the links to the OCR-files for each page of a source document. In the case of the BSB, the OCR-files are provided in XHTML from which the raw text of the transcriptions can easily be extracted and saved in TXT-files using XSLT.[^2] In our example, the TXT-files are stored in a directory named “corpus” with names containing information about their genre, printing year, a shortened title and the VD17-identifier which can be used to access the full metadata via the VD17-catalogue. The different bits of information are separated by underline characters resulting in file names like this one: “messrelation_1628_Relationis Hist_39_124466Q.txt”. Note that the colon in the VD17 identifier was for technical reasons also replaced by an underline character, converting the identifier “39:124466Q” to “39_124466Q”.

The data in our corpus can be used for stylometric analysis, as a reference corpus for text reuse detection and all sorts of quantitative analysis, aka distant reading.

[^1]: The prefix dc refers to the metadata standard [Dublin Core](https://www.dublincore.org/specifications/dublin-core/) which was specified at the end of the query URL. Dublin Core is sufficient for our use case, comparatively easy to understand, yet not very expressive and rich in information. For other use cases, other standards supported by the SRU-API of VD17, like [MARC21](https://www.loc.gov/marc/bibliographic/) or [PicaXML](https://verbundwiki.gbv.de/spaces/VZG/pages/35651588/PICA+XML+Version+1.0), are more appropriate, because they contain much more information. But they are also harder to understand and to work with, if you are not familiar with bibliographic metadata and cataloguing standards. See the [SRU-documentation for VD17](http://www.vd17.de/en/about-the-vd-17-online-catalogue/sru/).
[^2]: See for further reference the [API-documentation of the BSB](https://www.digitale-sammlungen.de/en/interfaces).
